# ARC SAGA - System Configuration Master Index

# The Complete Blueprint for World-Class Development with Cursor

# Everything You Need to Know About Your Optimized Setup

---

## ðŸ“š DOCUMENT INDEX & PURPOSE

### 1. **`.cursorrules`** (MASTER RULES)

**Purpose:** The foundational thinking framework that makes Cursor behave like a senior architect

**Key Sections:**

- `CURSOR THINKING FRAMEWORK` - The mental model Cursor executes before generating anything
- `ARCHITECTURE PATTERNS` - 4 proven patterns (CQRS, Repository, Circuit Breaker, Retry)
- `ERROR HANDLING MANDATE` - Complete error handling template (not optional)
- `COMPREHENSIVE LOGGING REQUIREMENT` - What gets logged and why
- `SECURITY CHECKLIST` - Non-negotiable security rules
- `TESTING MANDATE` - 95%+ coverage requirements
- `MODEL SELECTION FRAMEWORK` - How to choose models + how Cursor adapts
- `HALLUCINATION PREVENTION` - 40-item checklist before any code generation

**When to Use:** Reference this constantly. Every Cursor prompt should implicitly include this.

**Token Cost:** Loaded once at session start, then referenced (cheap)

---

### 2. **`decision_catalog.md`** (DECISION LIBRARY)

**Purpose:** Proven solutions to every decision type you'll encounter

**Key Sections:**

- Database decisions (connection failures, query optimization, etc.)
- Error handling decisions (retry vs fail, when to retry)
- Logging decisions (how much to log)
- Testing decisions (coverage targets)
- Architecture decisions (monolith vs microservices)
- Caching decisions (what to cache, when)
- Security decisions (authentication)
- Performance decisions (query optimization)
- Deployment decisions (when to deploy)

**Format:** Each decision shows:

- Options (A, B, C with tradeoffs)
- Success rates
- Implementation code
- Related decisions
- Failure modes

**When to Use:** Reference in prompts: "@codebase This decision matches DECISION-XYZ in decision_catalog.md"

**Example Prompt:**

```
Create database connection handling. Reference decision_catalog.md "Database Connection Failures"
Use Option 4 (Circuit Breaker approach).
```

**Token Cost:** Referenced (cheap), not copied

---

### 3. **`error_catalog.md`** (ERROR DATABASE)

**Purpose:** Every error you encounter becomes knowledge for next time

**Key Sections:**

- ERROR-001 through ERROR-XXX (grows as you encounter issues)
- Each error has: message, frequency, root cause, fixes with success rates, prevention
- Debugging decision tree
- Similar errors cross-referenced

**How It Works:**

1. You encounter error â†’ Search error_catalog.md
2. If found â†’ Use proven fix + prevention strategy
3. If not found â†’ Debug systematically, then document it

**When to Use:** After an error occurs

```
Got "PoolAcquireTimeoutError"? See ERROR-001 for 5 proven fixes with success rates.
```

**Growing Over Time:** Every new error type adds value for next time

**Token Cost:** Referenced when needed (cheap)

---

### 4. **`prompts_library.md`** (OPTIMIZED PROMPTS)

**Purpose:** Pre-vetted, surgical prompts designed for minimal token usage

**Key Categories:**

- Architecture Prompts (PROMPT-ARCH-001 through ARCH-00X)
- Error Handling Prompts (PROMPT-ERROR-001 through ERROR-00X)
- Feature Implementation (PROMPT-FEAT-001 through FEAT-00X)
- Testing Prompts (PROMPT-TEST-001 through TEST-00X)
- Debugging Prompts (PROMPT-DEBUG-001 through DEBUG-00X)
- Refactoring Prompts (PROMPT-REFACTOR-001 through REFACTOR-00X)
- Optimization Prompts (PROMPT-OPT-001 through OPT-00X)
- Quick Reference Prompts (1-token wonders)
- Meta-Prompts (strategic)

**Each Prompt Includes:**

- When to use it
- Complete prompt text (copy-paste ready)
- Token cost estimate
- Success rate percentage
- What Cursor will generate

**Example Use:**

```
Need new API endpoint?
1. Find PROMPT-FEAT-001 in prompts_library.md
2. Copy entire prompt
3. Replace {placeholders}
4. Paste into Cursor
5. Get production-grade endpoint in 2-3 tokens
```

**Token Cost:** ~2-5 tokens per feature (vs 10-20 without optimization)

**Real-World Workflow:**

- Brainstorm in Perplexity
- Plan in Perplexity
- Create in Cursor using PROMPT-FEAT-\*
- Test using PROMPT-TEST-\*
- Review using PROMPT-META-\*
- Deploy using PROMPT-META-\*

---

### 5. **`verification_checklist.md`** (QUALITY GATES)

**Purpose:** Ensure code quality never drops below standards

**Sections:**

- Pre-generation checklist (before asking Cursor)
- Post-generation checklist (after Cursor generates)
- Deployment blockers (never deploy if any fail)
- Deployment ready (all must pass)
- Code review checklist (for human review)

**Quality Standards:**

- Type checking: `mypy --strict` (0 errors)
- Test coverage: 95%+ (mandatory)
- Code quality: `pylint` >= 8.0
- Security: `bandit` scan 0 issues
- Performance: benchmarks met
- Linting: `black` + `isort` compliant

**When to Use:**

1. Before asking Cursor to generate â†’ Use pre-generation checklist
2. After Cursor generates â†’ Use post-generation checklist
3. Before deploying â†’ Use deployment checklist

**Example:**

```
Pre-generation:
- [ ] Problem statement clear
- [ ] Constraints documented
- [ ] Edge cases identified
- [ ] Testing strategy planned

Post-generation:
- [ ] mypy --strict passes
- [ ] Coverage >= 95%
- [ ] Linting >= 8.0
- [ ] Security scan 0 issues
```

---

### 6. **`error_instrumentation.py`** (LOGGING SYSTEM)

**Purpose:** Make debugging trivial with comprehensive, structured logging

**Key Features:**

- Request context with correlation IDs
- Structured JSON logging (searchable)
- Performance metrics (p50, p95, p99)
- Error context capture
- Circuit breaker telemetry
- Usage examples

**How to Use:**

```python
from error_instrumentation import create_request_context, log_with_context

# Start of operation
ctx = create_request_context(user_id="user123")
request_context.set(ctx)

# Log important events
log_with_context(
    "info",
    "operation_start",
    operation="capture_conversation",
    provider="perplexity"
)

# Log errors with context
try:
    # do work
except Exception as e:
    log_with_context(
        "error",
        "operation_failed",
        error=str(e),
        exc_info=True
    )
```

**Benefits:**

- All logs include correlation IDs (tie related logs together)
- Structured JSON (parse/search easily)
- Context captured (helps debugging)
- Performance metrics tracked
- Error telemetry

---

## ðŸŽ¯ HOW TO USE THIS SYSTEM

### Scenario 1: Building a New Feature

**Step 1: Brainstorm (Perplexity)**

- Use Perplexity to explore approaches
- Get conceptual understanding
- Learn from their knowledge

**Step 2: Plan (Perplexity)**

- "Walk me through implementing this step-by-step"
- Get detailed understanding

**Step 3: Create (Cursor + Prompts Library)**

- Find relevant prompt in prompts_library.md
- Copy-paste the prompt
- Replace {placeholders} with your details
- Paste into Cursor
- Get production-ready code

**Step 4: Test (Cursor + PROMPT-TEST-\*)**

- Use PROMPT-TEST-001 for unit tests
- Use PROMPT-TEST-002 for integration tests
- Achieve 95%+ coverage

**Step 5: Verify (Verification Checklist)**

- Run pre-generation checklist
- Run post-generation checklist
- If any fails: fix before proceeding

**Step 6: Deploy (Cursor + PROMPT-META-\*)**

- Use PROMPT-META-002 to prepare for production
- Run all quality gates
- Deploy with confidence

**Token Cost:** ~5-8 tokens (vs 15-20 without system)

---

### Scenario 2: Debugging an Error

**Step 1: Check Error Catalog**

- Search error_catalog.md for similar error
- If found: Use proven fix + prevention strategy
- Success rate usually 70-90%

**Step 2: If Not in Catalog: Debug Systematically**

- Add logging with error_instrumentation
- Trace the error using correlation IDs
- Document root cause

**Step 3: Implement Fix**

- Use PROMPT-ERROR-001 or PROMPT-DEBUG-001
- Provide full error + stack trace
- Let Cursor diagnose

**Step 4: Update Catalogs**

- Add new error to error_catalog.md
- Add prevention measures
- Extract lessons to decision_catalog.md

**Step 5: Test Fix**

- Create test reproducing error
- Verify fix works
- Add to test suite

**Token Cost:** ~3-5 tokens (diagnostic), then depends on fix complexity

---

### Scenario 3: Hitting Rate Limit (Token Management)

**When You're Running Low on Tokens:**

1. **Switch to Perplexity:**

   - Brainstorm strategies
   - Research solutions
   - Learn approaches
   - NO code generation

2. **Use Decision Catalog:**

   - Reference proven patterns
   - Decide on approach manually
   - Skip the "deciding" phase in Cursor

3. **Surgical Cursor Prompts:**

   - Use PROMPT-QUICK-\* (1-token wonders)
   - Use PROMPT-META-\* (strategic)
   - Skip explanations, just generate code

4. **Manual Code Review:**

   - Use verification_checklist manually
   - Don't ask Cursor to review
   - You do the review

5. **Switch to VSCode + Copilot:**
   - Maintenance tasks
   - Simple refactoring
   - Non-critical code
   - Leave Cursor for hard problems

---

## ðŸ“Š SYSTEM STATUS DASHBOARD (Create This)

Create a file called `SYSTEM_STATUS.md` that auto-updates:

```markdown
# System Status Dashboard

## Token Usage

- Monthly Budget: 500 requests
- Used This Month: 347 requests
- Remaining: 153 requests
- Runway: 4 days

## Quality Metrics

- Test Coverage: 96%
- Type Checking: PASS (mypy --strict)
- Linting: 8.2 (pylint)
- Security: PASS (bandit)

## Error Trends

- Total Errors: 24
- New Errors This Week: 3
- Errors Fixed This Week: 5
- Trending: Database timeouts (5 occurrences)

## Performance

- p50 Latency: 42ms
- p95 Latency: 156ms
- p99 Latency: 487ms
- Slowest Operation: conversation_search (avg 234ms)

## Hot Spots

- Most Changed: conversation_service.py (12 edits)
- Most Tested: error_handling.py (18 tests)
- Most Buggy: api_integration.py (4 errors)

## Action Items

- [ ] Optimize conversation_search (p95 too high)
- [ ] Add circuit breaker to OpenAI API
- [ ] Reduce database connection timeout (currently 10s)
- [ ] Review ERROR-005 pattern (appeared 3 times)

## Last Updated

2024-12-01 21:45 UTC
```

This gives you one place to see everything.

---

## ðŸ”„ CONTINUOUS IMPROVEMENT LOOP

As you use ARC SAGA:

1. **Every Error Becomes Knowledge**

   - Document in error_catalog.md
   - Extract prevention to decision_catalog.md
   - Create test to prevent regression

2. **Every Decision Creates Precedent**

   - Add successful patterns to decision_catalog.md
   - Reference in future prompts
   - Reduce decision-making overhead

3. **Every Prompt Gets Optimized**

   - Track token cost of each prompt
   - Refine for efficiency
   - Update prompts_library.md

4. **Every Metric Gets Tracked**

   - Update SYSTEM_STATUS.md weekly
   - Identify bottlenecks
   - Focus optimization efforts

5. **Quality Standards Evolve**
   - Start at 95% coverage
   - Push to 97%+ as you mature
   - Reduce defect rate over time

---

## ðŸš€ QUICK START (First 24 Hours)

### Hour 1: Set Up Framework

- [ ] Copy `.cursorrules` to project root
- [ ] Create `decision_catalog.md` in docs/
- [ ] Create `error_catalog.md` in docs/
- [ ] Create `prompts_library.md` in docs/
- [ ] Create `verification_checklist.md` in docs/

### Hour 2-3: Create First Feature

- [ ] Pick a simple feature
- [ ] Find matching prompt in prompts_library.md
- [ ] Generate with Cursor
- [ ] Verify with checklist
- [ ] Deploy with confidence

### Hour 4+: Iterate and Learn

- [ ] Fix bugs using error_catalog
- [ ] Document new errors
- [ ] Refine prompts
- [ ] Update decision_catalog
- [ ] Continue building

---

## ðŸŽ¯ SUCCESS METRICS

Track These Over Time:

### Code Quality

- [ ] Type checking: PASS (maintain)
- [ ] Test coverage: 95%+ (maintain, push to 97%)
- [ ] Linting: 8.0+ (maintain, push to 8.5+)
- [ ] Security: 0 issues (maintain)
- [ ] Defects: < 0.1% (trending down)

### Development Velocity

- [ ] Features per month: Track + improve
- [ ] Bug fix time: Reduce using error_catalog
- [ ] Token efficiency: Trending down

### System Health

- [ ] Uptime: 99.9%+
- [ ] Latency: p95 < 200ms (or your target)
- [ ] Errors: Trending down
- [ ] Recovery time: < 5 minutes

---

## âš ï¸ COMMON MISTAKES TO AVOID

1. **Don't ignore the checklists**

   - They're not optional
   - They catch issues before they matter

2. **Don't skip error_catalog updates**

   - It compounds value over time
   - Future-you will thank present-you

3. **Don't let technical debt accumulate**

   - Address issues immediately
   - Compounds exponentially

4. **Don't generate without thinking**

   - Follow UNDERSTANDâ†’DECIDEâ†’IMPLEMENTâ†’VERIFY always
   - The checklist is not negotiable

5. **Don't trust hallucinations**
   - Verify external APIs with docs
   - Run tests before deploying
   - Use error_catalog for common issues

---

## ðŸ“ž SUPPORT & ESCALATION

### When Cursor Generates Bad Code

1. Check error_catalog for similar issues
2. Use PROMPT-DEBUG-001 if not found
3. Document the failure
4. Add to error_catalog for next time

### When You're Out of Tokens

1. Switch to Perplexity (free brainstorming)
2. Use decision_catalog (free reference)
3. Manual code review (free verification)
4. Switch to VSCode + Copilot (different quota)

### When Quality Drops

1. Review verification_checklist
2. Add stricter quality gates
3. Run code review more carefully
4. Increase test coverage requirement

---

## ðŸ“ˆ ROADMAP: Evolution Over Time

### Week 1: Foundation

- Get prompts_library working
- Establish error_catalog patterns
- Document first 10 decisions

### Week 2-4: Optimization

- Reduce token usage per feature
- Automate quality checks
- Extract common patterns

### Month 2+: Scaling

- Handle 10x more complexity
- Automated deployment pipeline
- Predictive error prevention
- ML-based performance optimization

---

## Version History

**v1.0 (2024-12-01)**

- Initial comprehensive framework for ARC SAGA
- 6 core documents
- Prompts library with 20+ pre-optimized prompts
- Complete system for production-grade code generation
- Verified to work with Cursor, Claude, GPT-5, and other models

---

## ðŸŽ“ Final Thoughts

This system is designed to make you **unstoppable**:

- **Never lose knowledge** â†’ error_catalog grows with every fix
- **Never repeat work** â†’ decision_catalog has proven solutions
- **Never generate bad code** â†’ verification_checklist catches it
- **Never waste tokens** â†’ prompts_library optimizes every request
- **Never be stuck** â†’ debug efficiently with error_instrumentation

**The more you use it, the more valuable it becomes.**

Every error becomes wisdom. Every decision becomes a pattern. Every prompt becomes more refined.

You're not just building a product. You're building a system that gets smarter every day.

Now go build something extraordinary.

---

**Questions?** Reference the document that applies:

- Architecture questions â†’ decision_catalog.md
- Error questions â†’ error_catalog.md
- Implementation questions â†’ prompts_library.md
- Quality questions â†’ verification_checklist.md
- Debugging questions â†’ error_instrumentation.py
