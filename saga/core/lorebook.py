"""
LoreBook - Empirical Memory System
===================================

Balances Mimiry's idealism with project-specific learned patterns.
Stores decisions, tracks outcomes, extracts patterns.

Author: ARC SAGA Development Team
Date: December 17, 2025
Status: Phase 3A - Core Memory System
"""

import json
import logging
from dataclasses import asdict, dataclass, field
from datetime import datetime
from typing import Any, Literal, Optional
from uuid import uuid4

import tiktoken
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt, wait_exponential

from saga.resilience.async_utils import with_retry, with_timeout

logger = logging.getLogger(__name__)


# --- LLM Semantic Tagger Models (Pydantic) ---

class PersonaInfluence(BaseModel):
    """Tracks which Chameleon Role dominated the logic."""
    primary: str = "Unknown"
    rationale: str = ""


class LoreEntry(BaseModel):
    """
    Production-grade LoreBook entry with strict schema validation.
    Generated by LLM Semantic Tagger after governance activity.
    """
    entry_id: str = Field(default_factory=lambda: str(uuid4()))
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    semantic_tags: list[str] = Field(default_factory=list)
    persona_influence: PersonaInfluence = Field(default_factory=PersonaInfluence)
    codex_status: Literal["COMPLIANT", "VIOLATION", "NEUTRAL"] = "NEUTRAL"
    summary: str = ""  # "Lore Headline"

    # Audit & Debugging
    raw_llm_response: str | None = None
    tag_drift: list[str] = Field(default_factory=list)  # Discrepancies from heuristic

    # USMA Graph Bridge - links to code entities in RepoGraph
    related_entities: list[str] = Field(default_factory=list)  # Node IDs from graph

    class Config:
        json_encoders = {datetime: lambda v: v.isoformat()}


# Token counting utility
def count_tokens(text: str, model: str = "gpt-4") -> int:
    """Count tokens using tiktoken for accurate limits."""
    try:
        encoding = tiktoken.encoding_for_model(model)
        return len(encoding.encode(text))
    except Exception:
        # Fallback: ~4 chars per token
        return len(text) // 4


@dataclass
class Decision:
    """
    A persistent memory entry in the LoreBook (Structured JSON).

    Adheres to Phase 3.0 Precision Schema:
    {
      "timestamp": "ISO-8601",
      "trigger": "Error/Success/Deviation",
      "lesson": "Precision summary",
      "context": "File/Task metadata",
      "tags": ["Vectorized Keywords"],
      "user_comments": "Nested instructions/feedback"
    }
    """
    # Unique ID for storage
    decision_id: str = field(default_factory=lambda: str(uuid4()))

    # Precision Schema Fields
    timestamp: datetime = field(default_factory=datetime.utcnow)
    trigger: str = "Success" # Error/Success/Deviation
    lesson: str = ""         # "Precision summary" - what was learned
    context_str: str = ""    # "Contextual Metadata" serialized or summarized
    tags: list[str] = field(default_factory=list) # Vectorized Keywords
    user_comments: str = ""  # Feedback from HITL

    # Legacy / Internal Fields (maintained for compatibility/logic)
    question: str = ""              # Can map to "context" or be independent
    trace_id: str = ""
    raw_context: dict[str, Any] = field(default_factory=dict) # The actual dict context
    mimiry_ruling: str = ""
    lorebook_ruling: str = ""
    confidence: float = 1.0
    outcome_recorded: bool = False
    success: Optional[bool] = None

    def to_dict(self) -> dict[str, Any]:
        """Serialize to dict enforcing strict schema where possible."""
        data = asdict(self)
        data["timestamp"] = self.timestamp.isoformat()
        return data

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "Decision":
        """Deserialize from dict."""
        if "timestamp" in data and isinstance(data["timestamp"], str):
             data["timestamp"] = datetime.fromisoformat(data["timestamp"])
        return cls(**data)


@dataclass
class Outcome:
    """The result of a decision (success/failure + metrics)."""

    outcome_id: str = field(default_factory=lambda: str(uuid4()))
    decision_id: str = ""
    learned_at: datetime = field(default_factory=datetime.utcnow)

    # Results
    success: bool = False
    metrics: dict[str, Any] = field(default_factory=dict)  # test_coverage, errors, etc.
    feedback: str = ""                                      # What we learned

    def to_dict(self) -> dict[str, Any]:
        """Serialize to dict."""
        data = asdict(self)
        data["learned_at"] = self.learned_at.isoformat()
        return data

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "Outcome":
        """Deserialize from dict."""
        data["learned_at"] = datetime.fromisoformat(data["learned_at"])
        return cls(**data)


@dataclass
class Pattern:
    """An extracted pattern from multiple decisions."""

    pattern_id: str = field(default_factory=lambda: str(uuid4()))
    description: str = ""
    success_rate: float = 0.0
    examples: list[str] = field(default_factory=list)
    learned_from: list[str] = field(default_factory=list)  # decision_ids

    def to_dict(self) -> dict[str, Any]:
        """Serialize to dict."""
        return asdict(self)


class LoreBook:
    """
    Empirical Memory System for SAGA.

    Balances Mimiry's idealistic SagaCodex with project-specific learnings.
    Consult LoreBook FIRST (fast, local), then Mimiry (slower, ideal).

    Architecture:
        - DecisionStore: Persistent SQLite storage
        - VectorSearch: Similarity matching for past decisions
        - Confidence decay: Old decisions lose relevance
        - Pattern extraction: Generalize from multiple outcomes

    Usage:
        lorebook = LoreBook(project_root=Path.cwd())
        await lorebook.initialize()

        # Check memory first
        past_decision = await lorebook.consult("Should I use async Redis?", context)

        if past_decision and past_decision.confidence > 0.8:
            # Use learned approach
            ...
        else:
            # Fall back to Mimiry
            ...
    """

    def __init__(self, project_root: str = ".") -> None:
        """Initialize LoreBook for a project."""
        self.project_root = project_root
        self.store: Any = None  # DecisionStore (injected)
        self.vector_search: Any = None  # VectorSearch (injected)

        # Configuration
        self.confidence_decay_days = 90  # Decisions decay after 90 days
        self.similarity_threshold = 0.7   # Min similarity for match
        self.min_confidence = 0.6         # Min confidence to use

    async def initialize(self) -> None:
        """
        Initialize storage backend.

        Creates .saga/lorebook.db if not exists.
        Loads vector embeddings for existing decisions.
        """
        from saga.storage.decision_store import DecisionStore
        from saga.storage.vector_search import VectorSearch

        self.store = DecisionStore(self.project_root)
        await self.store.initialize()

        self.vector_search = VectorSearch()

        # Load existing decisions into vector search
        decisions = await self.store.get_all_decisions()
        for decision in decisions:
            self.vector_search.add_decision(decision)

        logger.info(
            "LoreBook initialized",
            extra={
                "project_root": self.project_root,
                "decisions_count": len(decisions)
            }
        )

    @with_timeout(10.0)
    @with_retry(max_attempts=3, backoff=1.0)
    async def consult(
        self,
        question: str,
        context: dict[str, Any],
        trace_id: str = ""
    ) -> Optional[Decision]:
        """
        Find similar past decision in LoreBook.

        Args:
            question: Question being asked
            context: Current context (framework, domain, etc.)
            trace_id: Distributed tracing ID

        Returns:
            Past decision if found with high confidence, else None

        Algorithm:
            1. Vector search for similar questions
            2. Apply confidence decay (old decisions lose confidence)
            3. Filter by context similarity
            4. Return best match if confidence > threshold
        """
        if not self.store or not self.vector_search:
            logger.warning("LoreBook not initialized")
            return None

        # Find similar decisions
        similar: list[tuple[Decision, float]] = self.vector_search.find_similar(
            question,
            context,
            top_k=5,
            threshold=self.similarity_threshold
        )

        if not similar:
            logger.debug(
                "No similar decisions found",
                extra={"question": question, "trace_id": trace_id}
            )
            return None

        # Apply confidence decay and filter
        best_decision: Optional[Decision] = None
        best_confidence: float = 0.0

        for decision, similarity_score in similar:
            # Decay confidence based on age
            age_days = (datetime.utcnow() - decision.timestamp).days
            decay_factor = max(0.0, 1.0 - (age_days / self.confidence_decay_days))

            adjusted_confidence = decision.confidence * decay_factor * similarity_score

            if adjusted_confidence > best_confidence and adjusted_confidence >= self.min_confidence:
                best_confidence = adjusted_confidence
                best_decision = decision

        if best_decision:
            # Return a copy with the effective (decayed) confidence
            # so the caller knows the current weight of this memory
            result = Decision.from_dict(best_decision.to_dict())
            result.confidence = best_confidence

            logger.info(
                "LoreBook match found",
                extra={
                    "question": question,
                    "decision_id": best_decision.decision_id,
                    "confidence": best_confidence,
                    "age_days": (datetime.utcnow() - best_decision.timestamp).days,
                    "trace_id": trace_id
                }
            )
            return result

        return None

    # --- LLM Semantic Tagger ---

    def _generate_heuristic_tags(self, text: str, context: dict[str, Any]) -> list[str]:
        """
        Shadow Tagger: Heuristic-based tag extraction for fallback and drift logging.
        Runs in parallel with LLM for comparison.
        """
        tags = set()
        content = (text + " " + str(context)).lower()

        keywords = {
            "fastapi": ["fastapi", "api", "endpoint"],
            "async": ["async", "await", "concurrency"],
            "database": ["db", "sql", "postgres", "sqlite", "sqlalchemy"],
            "testing": ["test", "pytest", "mock"],
            "security": ["auth", "login", "jwt", "token"],
            "refactoring": ["refactor", "cleanup", "structure"],
            "parallel": ["parallel", "worker", "concurrent", "thread"],
            "governance": ["governance", "proposal", "ledger", "approval"]
        }

        for tag, search_terms in keywords.items():
            if any(term in content for term in search_terms):
                tags.add(f"#{tag}")

        return list(tags)

    def _detect_persona_tension(
        self,
        alpha_output: dict[str, Any] | None,
        beta_output: dict[str, Any] | None
    ) -> tuple[bool, str]:
        """
        Detect conflicting logic between Worker Alpha (Coder) and Worker Beta (SDET).
        Returns (has_tension, rationale).
        """
        if not alpha_output or not beta_output:
            return False, ""

        # Simple conflict detection: check for error keywords in either
        alpha_str = str(alpha_output).lower()
        beta_str = str(beta_output).lower()

        conflict_signals = ["error", "failed", "conflict", "rejected", "override"]

        alpha_issues = any(signal in alpha_str for signal in conflict_signals)
        beta_issues = any(signal in beta_str for signal in conflict_signals)

        if alpha_issues and beta_issues:
            return True, "Both workers encountered issues - forced consensus"
        elif alpha_issues != beta_issues:
            return True, "Disagreement between Coder and SDET outputs"

        return False, ""

    def _build_semantic_prompt(
        self,
        ledger_content: str,
        persona_list: list[str],
        timestamp: str,
        recent_entries: list[dict[str, Any]] | None = None
    ) -> str:
        """Build the unified semantic tagger prompt."""
        context_anchor = ""
        if recent_entries:
            recent_tags = []
            for entry in recent_entries[:3]:
                recent_tags.extend(entry.get("semantic_tags", []))
            context_anchor = f"\n4. RECENT_LORE_TAGS (avoid redundancy): {recent_tags[:10]}"

        return f"""### ROLE
You are the SAGA Semantic Indexer. Your purpose is to analyze governance activity and generate structured metadata for the LoreBook.

### INPUT DATA
1. PROPOSAL_CONTENT: {ledger_content}
2. ACTIVE_PERSONAS: {persona_list}
3. TIMESTAMP: {timestamp}{context_anchor}

### CONSTRAINTS
- Use ISO-8601 for all temporal references.
- Categorize based on the "Parallel Sovereignty" architecture.
- Identify if the content aligns with or violates the current Codex.
- Output MUST be valid JSON matching the schema below.

### TAGGING TAXONOMY
Generate tags across three tiers:
1. **Functional**: (e.g., #refactor, #security, #deployment)
2. **Persona-Driven**: Based on which Chameleon Roles dominated (e.g., #auditor-verified, #architect-design)
3. **Lore-Specific**: Identifying thematic impact on "SAGA Phase 3.0"

### REQUIRED OUTPUT FORMAT (JSON ONLY)
{{
  "semantic_tags": ["#tag1", "#tag2", "#tag3"],
  "persona_influence": {{
    "primary": "PersonaName",
    "rationale": "Brief explanation of persona impact"
  }},
  "codex_status": "COMPLIANT",
  "summary": "One sentence summary of the sovereignty shift."
}}"""

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        reraise=True
    )
    async def _call_llm_for_tags(self, prompt: str) -> dict[str, Any]:
        """Call LLM with exponential backoff retry logic."""
        if not self.llm_client:
            raise RuntimeError("LLM client not initialized")

        # Token check - summarize if too long
        token_count = count_tokens(prompt)
        if token_count > 2000:
            logger.warning(f"Prompt too long ({token_count} tokens), will truncate")
            prompt = prompt[:8000]  # Rough char limit

        response = await self.llm_client.complete(
            prompt=prompt,
            max_tokens=500,
            temperature=0.3
        )

        # Parse JSON from response
        try:
            # Extract JSON from potential markdown code blocks
            content = response.content if hasattr(response, 'content') else str(response)
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]

            return json.loads(content.strip())
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM JSON response: {e}")
            raise

    async def generate_lore_entry(
        self,
        ledger_content: str,
        persona_list: list[str],
        alpha_output: dict[str, Any] | None = None,
        beta_output: dict[str, Any] | None = None,
        recent_entries: list[dict[str, Any]] | None = None
    ) -> LoreEntry:
        """
        Generate a complete LoreEntry using LLM with Shadow Tagger fallback.

        Features:
        - LLM-based semantic tagging with retry
        - Shadow Tagger for fallback + drift detection
        - Persona tension detection
        - Audit trail via raw_llm_response
        """
        timestamp = datetime.utcnow().isoformat()

        # Always run heuristic (Shadow Tagger)
        heuristic_tags = self._generate_heuristic_tags(ledger_content, {
            "alpha": alpha_output,
            "beta": beta_output
        })

        # Detect persona tension
        has_tension, tension_rationale = self._detect_persona_tension(alpha_output, beta_output)
        if has_tension:
            heuristic_tags.append("#persona-friction")

        # Try LLM tagging
        llm_result: dict[str, Any] | None = None
        raw_response: str | None = None

        try:
            prompt = self._build_semantic_prompt(
                ledger_content, persona_list, timestamp, recent_entries
            )
            llm_result = await self._call_llm_for_tags(prompt)
            raw_response = json.dumps(llm_result)
        except Exception as e:
            logger.warning(f"LLM tagging failed, using heuristic fallback: {e}")

        # Build LoreEntry
        if llm_result:
            llm_tags = llm_result.get("semantic_tags", [])

            # Calculate tag drift
            drift = [t for t in heuristic_tags if t not in llm_tags]
            if drift:
                logger.info(f"Tag drift detected: {drift}")

            entry = LoreEntry(
                timestamp=datetime.utcnow(),
                semantic_tags=llm_tags,
                persona_influence=PersonaInfluence(
                    primary=llm_result.get("persona_influence", {}).get("primary", "Unknown"),
                    rationale=llm_result.get("persona_influence", {}).get("rationale", "")
                ),
                codex_status=llm_result.get("codex_status", "NEUTRAL"),
                summary=llm_result.get("summary", ""),
                raw_llm_response=raw_response,
                tag_drift=drift
            )
        else:
            # Fallback to heuristic
            entry = LoreEntry(
                timestamp=datetime.utcnow(),
                semantic_tags=heuristic_tags,
                persona_influence=PersonaInfluence(
                    primary="Unknown",
                    rationale=tension_rationale if has_tension else "Heuristic fallback - LLM unavailable"
                ),
                codex_status="NEUTRAL",
                summary="Auto-generated via heuristic tagger",
                raw_llm_response=None,
                tag_drift=[]
            )

        return entry

    async def _generate_semantic_tags(self, text: str, context: dict[str, Any]) -> list[str]:
        """
        Generate semantic tags - now uses LLM with heuristic fallback.
        Maintains backward compatibility with existing code.
        """
        try:
            entry = await self.generate_lore_entry(
                ledger_content=text,
                persona_list=["Coder", "SDET"],  # Default personas
                alpha_output=context.get("alpha"),
                beta_output=context.get("beta")
            )
            return entry.semantic_tags
        except Exception as e:
            logger.warning(f"Full LoreEntry generation failed: {e}")
            return self._generate_heuristic_tags(text, context)

    @with_timeout(5.0)
    async def record_decision(self, decision: Decision) -> None:
        """
        Store a new decision in LoreBook.

        Args:
            decision: Decision to record

        Side effects:
            - Auto-generates tags if missing
            - Persists to SQLite
            - Adds to vector search index
        """
        if not self.store or not self.vector_search:
            logger.error("LoreBook not initialized")
            return

        # Auto-generate tags if missing
        if not decision.tags:
            decision.tags = await self._generate_semantic_tags(
                decision.question or decision.lesson,
                decision.raw_context
            )

        await self.store.save_decision(decision)
        self.vector_search.add_decision(decision)

        logger.info(
            "Decision recorded",
            extra={
                "decision_id": decision.decision_id,
                "question": decision.question,
                "tags": decision.tags,
                "diverged_from_mimiry": decision.mimiry_ruling != decision.lorebook_ruling,
                "trace_id": decision.trace_id
            }
        )

    @with_timeout(5.0)
    async def record_outcome(self, outcome: Outcome) -> None:
        """
        Record the outcome of a decision (learn from results).

        Args:
            outcome: Outcome to record

        Side effects:
            - Updates decision.success and decision.confidence
            - If failure, reduces confidence
            - If success, increases confidence (up to 1.0)
        """
        if not self.store:
            logger.error("LoreBook not initialized")
            return

        await self.store.save_outcome(outcome)

        # Update decision based on outcome
        decision = await self.store.get_decision(outcome.decision_id)
        if decision:
            decision.outcome_recorded = True
            decision.success = outcome.success

            # Adjust confidence based on outcome
            if outcome.success:
                decision.confidence = min(1.0, decision.confidence * 1.1)  # Boost
            else:
                decision.confidence = max(0.0, decision.confidence * 0.7)  # Penalize

            await self.store.update_decision(decision)

            logger.info(
                "Outcome recorded",
                extra={
                    "outcome_id": outcome.outcome_id,
                    "decision_id": outcome.decision_id,
                    "success": outcome.success,
                    "new_confidence": decision.confidence
                }
            )

    async def get_project_patterns(self) -> list[Pattern]:
        """
        Extract learned patterns from multiple decisions.

        Returns:
            List of patterns discovered (e.g., "Always use async Redis")

        Algorithm:
            1. Group decisions by tags/context
            2. Calculate success rate per group
            3. Generalize into patterns
        """
        if not self.store:
            return []

        decisions = await self.store.get_all_decisions()
        outcomes = await self.store.get_all_outcomes()

        # Map outcomes to decisions only if needed, currently unused in this simplified logic
        # outcome_map = {o.decision_id: o for o in outcomes}

        # Group by tags
        tag_groups: dict[str, list[Decision]] = {}
        for decision in decisions:
            for tag in decision.tags:
                tag_groups.setdefault(tag, []).append(decision)

        # Extract patterns
        patterns: list[Pattern] = []

        for tag, decisions_with_tag in tag_groups.items():
            if len(decisions_with_tag) < 3:  # Need multiple examples
                continue

            successes = sum(
                1 for d in decisions_with_tag
                if d.outcome_recorded and d.success
            )
            total = sum(1 for d in decisions_with_tag if d.outcome_recorded)

            if total > 0:
                success_rate = successes / total

                if success_rate > 0.7:  # High success pattern
                    pattern = Pattern(
                        description=f"Pattern: {tag} (works well in this project)",
                        success_rate=success_rate,
                        examples=[d.lorebook_ruling for d in decisions_with_tag[:3]],
                        learned_from=[d.decision_id for d in decisions_with_tag]
                    )
                    patterns.append(pattern)

        logger.info(
            "Patterns extracted",
            extra={"patterns_count": len(patterns)}
        )

        return patterns

    async def get_decision_history(
        self,
        limit: int = 10,
        tag: Optional[str] = None
    ) -> list[Decision]:
        """Get recent decisions, optionally filtered by tag."""
        if not self.store:
            return []

        return await self.store.get_recent_decisions(limit=limit, tag=tag)  # type: ignore

    # --- Mythos Chronicler ---

    async def consolidate_to_mythos(
        self,
        min_entries: int = 20,
        max_entries: int = 50
    ) -> Any:  # Returns MythosChapter | None
        """
        Consolidate recent LoreEntries into a Mythos Chapter.

        Performs LLM-based "Historical Review" to extract:
        - Universal principles
        - Solved patterns
        - Architectural debt

        Args:
            min_entries: Minimum entries before consolidation triggers
            max_entries: Maximum entries to include in one chapter

        Returns:
            MythosChapter if consolidation occurred, None otherwise
        """
        from saga.core.memory.mythos import (
            ArchitecturalDebt,
            MythosChapter,
            SolvedPattern,
        )

        if not self.store:
            logger.warning("Cannot consolidate: store not initialized")
            return None

        # Get unconsolidated entries
        decisions = await self.store.get_all_decisions()

        # Filter to recent, unprocessed entries
        # In production, we'd track which entries have been consolidated
        if len(decisions) < min_entries:
            logger.info(f"Not enough entries for consolidation ({len(decisions)} < {min_entries})")
            return None

        entries_to_consolidate = decisions[:max_entries]

        # Build consolidation prompt
        entry_summaries = []
        entry_ids = []
        tags_seen: set[str] = set()

        for decision in entries_to_consolidate:
            entry_summaries.append(f"- {decision.lesson or decision.question}")
            entry_ids.append(decision.decision_id)
            tags_seen.update(decision.tags)

        prompt = self._build_mythos_prompt(entry_summaries, list(tags_seen))

        # Try LLM consolidation
        mythos_data: dict | None = None
        if self.llm_client:
            try:
                response = await self.llm_client.complete(
                    prompt=prompt,
                    max_tokens=1000,
                    temperature=0.4
                )
                content = response.content if hasattr(response, 'content') else str(response)

                # Parse JSON response
                if "```json" in content:
                    content = content.split("```json")[1].split("```")[0]
                elif "```" in content:
                    content = content.split("```")[1].split("```")[0]

                mythos_data = json.loads(content.strip())
            except Exception as e:
                logger.warning(f"LLM Mythos consolidation failed: {e}")

        # Build chapter (with or without LLM)
        if mythos_data:
            solved_patterns = [
                SolvedPattern(
                    name=p.get("name", ""),
                    description=p.get("description", ""),
                    example=p.get("example", "")
                )
                for p in mythos_data.get("solved_patterns", [])
            ]

            architectural_debt = [
                ArchitecturalDebt(
                    name=d.get("name", ""),
                    description=d.get("description", ""),
                    severity=d.get("severity", "MEDIUM"),
                    suggested_fix=d.get("suggested_fix", "")
                )
                for d in mythos_data.get("architectural_debt", [])
            ]

            chapter = MythosChapter(
                title=mythos_data.get("title", f"Chapter {datetime.utcnow().strftime('%Y-%m-%d')}"),
                summary=mythos_data.get("summary", ""),
                solved_patterns=solved_patterns,
                architectural_debt=architectural_debt,
                universal_principles=mythos_data.get("universal_principles", []),
                lore_entry_ids=entry_ids,
                entry_count=len(entries_to_consolidate),
                tags=list(tags_seen)[:10]
            )
        else:
            # Fallback: simple aggregation
            chapter = MythosChapter(
                title=f"Development Log {datetime.utcnow().strftime('%Y-%m-%d')}",
                summary=f"Consolidated {len(entries_to_consolidate)} development decisions.",
                universal_principles=[f"Tag pattern: {tag}" for tag in list(tags_seen)[:5]],
                lore_entry_ids=entry_ids,
                entry_count=len(entries_to_consolidate),
                tags=list(tags_seen)[:10]
            )

        logger.info(f"Mythos chapter created: {chapter.title} ({chapter.entry_count} entries)")
        return chapter

    def _build_mythos_prompt(self, entry_summaries: list[str], tags: list[str]) -> str:
        """Build the LLM prompt for Mythos consolidation."""
        entries_text = "\n".join(entry_summaries[:30])  # Limit for tokens

        return f"""### ROLE
You are the SAGA Mythos Chronicler. Your purpose is to consolidate development decisions into lasting project wisdom.

### INPUT DATA
**Recent Development Decisions:**
{entries_text}

**Common Tags:** {tags[:10]}

### TASK
Analyze these decisions and extract:
1. **Universal Principles** - Rules that should always be followed
2. **Solved Patterns** - Reusable solutions that worked
3. **Architectural Debt** - Anti-patterns or issues to avoid

### REQUIRED OUTPUT FORMAT (JSON ONLY)
{{
  "title": "Chapter title reflecting the development phase",
  "summary": "2-3 sentence narrative of what was accomplished",
  "universal_principles": ["Principle 1", "Principle 2"],
  "solved_patterns": [
    {{"name": "Pattern Name", "description": "What it solves", "example": "Brief example"}}
  ],
  "architectural_debt": [
    {{"name": "Debt Name", "description": "The problem", "severity": "LOW|MEDIUM|HIGH|CRITICAL", "suggested_fix": "How to fix"}}
  ]
}}"""

    async def close(self) -> None:
        """Close storage connections."""
        if self.store:
            await self.store.close()

